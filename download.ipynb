{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Downloading Omni6DPose Dataset\n",
    "Omni6DPose is a dataset for universal 6D object pose estimation and tracking, featured by its diversity in object categories, large scale, and variety in object materials. The project is available at [Omni6DPose](https://jiyao06.github.io/Omni6DPose/). In this tutorial, we will show you how to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################ Set the configuration for loading data ##########################\n",
    "DATA_LINKS = 'data_links.json'   # Path to the json file containing the download links\n",
    "DATA_PATH = 'data/Omni6DPose/'           # Path to the directory where the data will be stored\n",
    "##############################################################################################\n",
    "\n",
    "import json, os, sys, requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get the download links and create the data directory\n",
    "with open(DATA_LINKS) as f:\n",
    "    data = json.load(f)\n",
    "for key in data:\n",
    "    os.makedirs(DATA_PATH + key, exist_ok=True)\n",
    "\n",
    "# Define the download function\n",
    "def download_file(url, filename):\n",
    "    progress_bar = None\n",
    "    try:\n",
    "        if os.path.exists(filename) :\n",
    "            print(f\"{filename} already exists!\")\n",
    "            return True\n",
    "        else:\n",
    "            print('\\033[93m' + f\"Downloading {filename}...\" + '\\033[0m')\n",
    "        url = url.replace('dl=0', 'dl=1')\n",
    "        response = requests.get(url, stream=True)     \n",
    "        total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)   \n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    progress_bar.update(len(chunk))\n",
    "                    f.write(chunk)\n",
    "            progress_bar.close()\n",
    "            return True\n",
    "        else:\n",
    "            print('\\033[91m' + f\"Failed to download {filename}! The server returned status code {response.status_code}\" + '\\033[0m')\n",
    "            return False\n",
    "    \n",
    "    except (KeyboardInterrupt, requests.RequestException, Exception) as e:\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        print('\\033[91m' + f\"An error occurred while downloading {filename}! {e} The file has been removed.\" + '\\033[0m')\n",
    "        return False\n",
    "    \n",
    "    finally:\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 1. Download Meta files\n",
    "    The Meta files are available at [Meta](https://www.dropbox.com/scl/fo/xumdcn3yp4hi0ahrg0v23/AMEANjZJoDpg5eeccQ02KnQ?rlkey=csw09o0uoh2q6ia93wcpp3mea&st=sru933q5&dl=0). You can download the files using the following command. The files is about 4MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta_links = data['Meta']\n",
    "for item, link in Meta_links.items():\n",
    "    print('\\033[93m' + f\"Downloading {item} ...\" + '\\033[0m')\n",
    "    os.makedirs(DATA_PATH + 'Meta', exist_ok=True)\n",
    "    download_file(link, f\"{DATA_PATH}Meta/{item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 2. Download PAM dataset [Optional]\n",
    "    The PAM dataset is available at [PAM](https://www.dropbox.com/scl/fo/oxpad3hjijwehrbdp0krf/AHaWA2xhz0lN-x6elq3Sf2I?rlkey=sctuyqczvmqukth4z4lqk2uj7&st=xx2xpwky&dl=0). You can download the dataset using the following command. The dataset is about 14GB. PAM contains the pose aligned 3D models, which is not necessary for the benchmarking if your method does not require the reference 3D models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAM_links = data['PAM']\n",
    "for item, link in PAM_links.items():\n",
    "    print('\\033[93m' + f\"Downloading {item} ...\" + '\\033[0m')\n",
    "    os.makedirs(DATA_PATH + 'PAM', exist_ok=True)\n",
    "    download_file(link, f\"{DATA_PATH}PAM/{item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 3. Download ROPE dataset\n",
    "    The ROPE dataset is available at [ROPE](https://www.dropbox.com/scl/fo/). You can download the dataset using the following command. The dataset contains 363 scenes, with each scene being approximately 500MB. The total size of the dataset is about 180GB. If you want to download a subset of the dataset, you can set the parameter `SCENE_NUM` in the following scripts. If the download is interrupted, you can rerun the script, and it will skip the downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Set the configuration for loading the ROPE data ###################################\n",
    "FILES = ['meta.zip', 'color.zip', 'depth.zip', 'mask.zip', 'mask_sam.zip']   # The files to be downloaded\n",
    "SCENE_NUM = 2                                                                # The number of scenes to be downloaded\n",
    "#######################################################################################################################\n",
    "\n",
    "assert SCENE_NUM > 0 and SCENE_NUM <= 363, 'Invalid scene number'\n",
    "ROPE_links = data['ROPE']\n",
    "scenes = list(ROPE_links.keys())\n",
    "scene_ids = [scene.split('-')[0] for scene in scenes]\n",
    "scene_ids = sorted(list(set(scene_ids)))[:SCENE_NUM]\n",
    "print(f\"The selected scenes are: {scene_ids}\")\n",
    "\n",
    "for scene in scene_ids:\n",
    "    print('\\033[93m' + f\"Downloading scene {scene} ...\" + '\\033[0m')\n",
    "    os.makedirs(DATA_PATH + 'ROPE/' + scene, exist_ok=True)\n",
    "    for file in FILES:\n",
    "        download_file(ROPE_links[f\"{scene}-{file}\"], f\"{DATA_PATH}ROPE/{scene}/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 4. Download SOPE dataset\n",
    "    The SOPE dataset is available at [SOPE](). You can download the dataset using the following command. The dataset comprises 51 patches. If you want to download a subset of the dataset, you can set the `PATCH_NUM` parameter in the scripts below. If the download is interrupted, you can rerun the script, and it will skip the downloaded files.\n",
    "    \n",
    "    The files `ir.zip`, `depth_1.zip`, and `coord.zip` are not necessary for benchmarking. Specifically, `ir.zip` contains the simulated infrared images, `depth_1.zip` includes the perfect depth images, and `coord.zip` contains the NOCS map proposed in [NOCS](). If you wish to download these files, you can set the `DOWNLOAD_IR`, `DOWNLOAD_GT_DEPTH`, and `DOWNLOAD_NOCS` parameters to `True` in the scripts below. Each batch of the dataset without the optional files is about 14GB. The total size of the dataset is about 720GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Set the configuration for loading the SOPE data ############################################\n",
    "FILES = ['meta.zip', 'color.zip', 'depth.zip', 'depth_1.zip', 'mask.zip', 'coord.zip', 'ir.zip'] # The selected files to be downloaded\n",
    "DOWNLOAD_IR = False                                                                              # Download the IR images\n",
    "DOWNLOAD_GT_DEPTH = False                                                                        # Download the perfect depth images\n",
    "DOWNLOAD_NOCS = False                                                                            # Download the NOCS map\n",
    "PATCH_NUM = 2                                                                                    # The number of patches to be downloaded\n",
    "#########################################################################################################################################\n",
    "\n",
    "FILES.remove('ir.zip') if not DOWNLOAD_IR else None\n",
    "FILES.remove('depth_1.zip') if not DOWNLOAD_GT_DEPTH else None\n",
    "FILES.remove('coord.zip') if not DOWNLOAD_NOCS else None\n",
    "\n",
    "assert PATCH_NUM > 0 and PATCH_NUM <= 50, 'Invalid patch number'\n",
    "SOPE_links = data['SOPE']\n",
    "patches = list(SOPE_links.keys())\n",
    "patch_ids = [patch.split('-')[0] for patch in patches]\n",
    "patch_ids = sorted(list(set(patch_ids)))[:PATCH_NUM]\n",
    "print('\\033[93m' + f\"The total number of patches is: {len(patch_ids)}\" + '\\033[0m')\n",
    "print('\\033[93m' +  f'The selected patches are: {patch_ids}' + '\\033[0m')\n",
    "\n",
    "for patch in patch_ids:\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    print('\\033[93m' + f\"Downloading patch {patch} ...\" + '\\033[0m')\n",
    "    os.makedirs(DATA_PATH + 'SOPE/' + patch + '/train', exist_ok=True)\n",
    "    os.makedirs(DATA_PATH + 'SOPE/' + patch + '/test', exist_ok=True)\n",
    "    for file in FILES:\n",
    "        status = download_file(SOPE_links[f\"{patch}-{'train'}-{file}\"], f\"{DATA_PATH}SOPE/{patch}/train/{file}\")\n",
    "        sys.exit() if not status else None\n",
    "        status = download_file(SOPE_links[f\"{patch}-{'test'}-{file}\"], f\"{DATA_PATH}SOPE/{patch}/test/{file}\")\n",
    "        sys.exit() if not status else None\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "print('\\033[92m' + \"All data downloaded successfully!\" + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 5. Unzip the files\n",
    "    After downloading the files, you can unzip the files using the following command. If you want to remove the zip files after unzipping, you can set the `REMOVE_ZIP` parameter to `True` in the scripts below. The files will be unzipped to the `./data` directory, and the directory structure will be as follows:\n",
    "    ```\n",
    "    data\n",
    "    └── Omni6DPose\n",
    "        ├── ROPE\n",
    "        │   ├── SCENE_ID\n",
    "        │   │   ├── FRAME_ID_meta.json\n",
    "        │   │   ├── FRAME_ID_color.png\n",
    "        │   │   ├── FRAME_ID_mask.exr\n",
    "        │   │   ├── FRAME_ID_depth.exr\n",
    "        │   │   ├── FRAME_ID_mask_sam.npz [Optional]\n",
    "        │   │   └── ...\n",
    "        │   └── ...\n",
    "        ├── SOPE\n",
    "        │   ├── PATCH_ID\n",
    "        │   │   ├── train\n",
    "        │   │   │   ├── SCENE_NAME\n",
    "        │   │   │   |  ├── SCENE_ID\n",
    "        │   |   |   |  |   ├── FRAME_ID_meta.json\n",
    "        │   │   │   |  |   ├── FRAME_ID_color.png\n",
    "        │   │   │   |  |   ├── FRAME_ID_mask.exr\n",
    "        │   |   |   |  |   ├── FRAME_ID_depth.exr\n",
    "        │   │   │   |  |   ├── FRAME_ID_depth_1.exr [Optional]\n",
    "        │   │   │   |  |   ├── FRAME_ID_coord.png   [Optional]\n",
    "        │   │   │   |  |   ├── FRAME_ID_ir_l.png    [Optional]\n",
    "        │   │   │   |  |   ├── FRAME_ID_ir_r.png    [Optional]\n",
    "        │   │   │   |  |   └── ...\n",
    "        │   │   │   |  └── ...\n",
    "        │   │   │   └── ...\n",
    "        │   │   └── test\n",
    "        │   └── ...\n",
    "        ├── PAM\n",
    "        │   └── obj_meshes\n",
    "        │       ├── DATASET-CLASS_ID\n",
    "        │       └── ...\n",
    "        └── Meta    \n",
    "            ├── obj_meta.json \n",
    "            └── real_obj_meta.json\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "################################### Set the configuration for unzipping the downloaded files ##################################\n",
    "REMOVE_ZIPS = True   # Remove the zip files after unzipping\n",
    "UNZIP_PAM = True     # Unzip the PAM files\n",
    "UNZIP_ROPE = True    # Unzip the ROPE files\n",
    "UNZIP_SOPE = True    # Unzip the SOPE files\n",
    "###############################################################################################################################\n",
    "\n",
    "def unzip_files(directory, remove_zips=False):\n",
    "    files = os.listdir(directory)\n",
    "    for file in files:\n",
    "        if file.endswith('.zip'):\n",
    "            with zipfile.ZipFile(directory + file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(directory)\n",
    "            if remove_zips:\n",
    "                os.remove(directory + file)\n",
    "\n",
    "# Unzip the PAM files\n",
    "if UNZIP_PAM:\n",
    "    print('\\033[93m' + f\"Unzipping files in {DATA_PATH + 'PAM/'} ...\" + '\\033[0m')\n",
    "    unzip_files(DATA_PATH + 'PAM/', remove_zips=REMOVE_ZIPS)\n",
    "\n",
    "# Unzip the ROPE files\n",
    "if UNZIP_ROPE:\n",
    "    scenes = os.listdir(DATA_PATH + 'ROPE/')\n",
    "    print('\\033[93m' + f\"Unzipping files in {DATA_PATH + 'ROPE/'} ...\" + '\\033[0m')\n",
    "    for scene in tqdm(scenes):\n",
    "        unzip_files(DATA_PATH + 'ROPE/' + scene + '/', remove_zips=REMOVE_ZIPS)\n",
    "    \n",
    "# Unzip the SOPE files\n",
    "if UNZIP_SOPE:\n",
    "    patches = os.listdir(DATA_PATH + 'SOPE/')\n",
    "    print('\\033[93m' + f\"Unzipping files in {DATA_PATH + 'SOPE/'} ...\" + '\\033[0m')\n",
    "    for patch in tqdm(patches):\n",
    "        print('\\033[93m' + f\"Unzipping files in {DATA_PATH + 'SOPE/' + patch} ...\" + '\\033[0m')\n",
    "        unzip_files(DATA_PATH + 'SOPE/' + patch + '/train/', remove_zips=REMOVE_ZIPS)\n",
    "        unzip_files(DATA_PATH + 'SOPE/' + patch + '/test/', remove_zips=REMOVE_ZIPS)\n",
    "    print('\\033[92m' + \"All files unzipped successfully!\" + '\\033[0m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omni6dpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
