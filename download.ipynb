{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Downloading Omni6DPose Dataset\n",
    "Omni6DPose is a dataset for universal 6D object pose estimation and tracking, featured by its diversity in object categories, large scale, and variety in object materials. The project is available at [Omni6DPose](https://jiyao06.github.io/Omni6DPose/). In this tutorial, we will show you how to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up the download configuration\n",
    " We need to set up the download configuration first. `DATA_PATH` is the path to save the dataset. **Set the `PROXY` to your own proxy if you are behind a firewall. If you are not behind a firewall, you can set `PROXY` to `None`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Set the configuration for loading data ###########################\n",
    "DATA_PATH = 'data/Omni6DPose/'              # Path to the directory where the data will be stored\n",
    "PROXY = 'http://proxy-server:proxy-port'    # Proxy server to use for downloading the data\n",
    "#################################################################################################\n",
    "\n",
    "import json, os, sys, requests, zipfile\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "# Set the proxy server\n",
    "proxies = {'http': PROXY, 'https': PROXY}\n",
    "\n",
    "# Get the download links and create the data directory\n",
    "with open('data_links.json') as f:\n",
    "    data = json.load(f)\n",
    "for key in data:\n",
    "    os.makedirs(DATA_PATH + key, exist_ok=True)\n",
    "\n",
    "\n",
    "def check_zip_file_integrity(zip_path):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "            bad_file = zip_file.testzip()\n",
    "            if bad_file:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    except zipfile.BadZipFile:\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_with_retries(url, proxies=None, timeout=5, max_retries=5, retry_delay=2):\n",
    "    \"\"\"\n",
    "    Attempts to download content with a retry mechanism on timeout or connection errors.\n",
    "    \n",
    "    Parameters:\n",
    "    - url (str): The URL of the resource to download.\n",
    "    - proxies (dict): Proxy settings.\n",
    "    - timeout (int or float): Request timeout in seconds.\n",
    "    - max_retries (int): Maximum number of retries.\n",
    "    - retry_delay (int or float): Delay between retries in seconds.\n",
    "    \n",
    "    Returns:\n",
    "    - response: The Response object if successful, None if all retries fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, proxies=proxies, timeout=timeout)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            return response  # Successful response, return the response object\n",
    "        except (requests.Timeout, requests.ConnectionError):\n",
    "            print(f\"Request timed out or connection error, attempting retry {attempt + 1}...\")\n",
    "            sleep(retry_delay)\n",
    "        except requests.HTTPError as err:\n",
    "            print(f\"HTTP error: {err}\")\n",
    "            return None  # On HTTP error, return None\n",
    "    print(\"All retry attempts failed\")\n",
    "    return None  # All retries failed, return None\n",
    "\n",
    "\n",
    "def download_file(url, filename):\n",
    "    progress_bar = None\n",
    "    try:\n",
    "        if os.path.exists(filename) :\n",
    "            if filename.endswith('.zip') and not check_zip_file_integrity(filename):\n",
    "                print('\\033[93m' + f\"{filename} already exists but is corrupted! Re-downloading...\" + '\\033[0m')\n",
    "            else:\n",
    "                print(f\"{filename} already exists!\")\n",
    "                return True\n",
    "        else:\n",
    "            print('\\033[93m' + f\"Downloading {filename}...\" + '\\033[0m')\n",
    "        url = url.replace('dl=0', 'dl=1')\n",
    "        # response = requests.get(url, stream=True, proxies=proxies)     \n",
    "        response = download_with_retries(url, proxies=proxies)\n",
    "        total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)   \n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    progress_bar.update(len(chunk))\n",
    "                    f.write(chunk)\n",
    "            progress_bar.close()\n",
    "            return True\n",
    "        else:\n",
    "            print('\\033[91m' + f\"Failed to download {filename}! The server returned status code {response.status_code}\" + '\\033[0m')\n",
    "            return False\n",
    "    \n",
    "    except (KeyboardInterrupt, requests.RequestException, Exception) as e:\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        print('\\033[91m' + f\"An error occurred while downloading {filename}! {e} The file has been removed.\" + '\\033[0m')\n",
    "        return False\n",
    "    \n",
    "    finally:\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download Meta files\n",
    "The Meta files are available at [Meta](https://www.dropbox.com/scl/fo/xumdcn3yp4hi0ahrg0v23/AMEANjZJoDpg5eeccQ02KnQ?rlkey=csw09o0uoh2q6ia93wcpp3mea&st=sru933q5&dl=0). You can download the files using the following command. The files is about 4MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta_links = data['Meta']\n",
    "fail_list = []\n",
    "for item, link in Meta_links.items():\n",
    "    print('\\033[93m' + f\"Downloading {item} ...\" + '\\033[0m')\n",
    "    os.makedirs(DATA_PATH + 'Meta', exist_ok=True)\n",
    "    res = download_file(link, f\"{DATA_PATH}Meta/{item}\")\n",
    "    if not res:\n",
    "        fail_list.append(item)\n",
    "if len(fail_list) == 0:\n",
    "    print('\\033[92m' + \"Meta data downloaded successfully!\" + '\\033[0m')\n",
    "else:\n",
    "    print('\\n' + '-'*100)\n",
    "    print('\\033[91m' + \"Meta data download failed for the following files:\" + '\\033[0m')\n",
    "    print(fail_list)\n",
    "    print('\\033[91m' + \"Please rerun the cell to retry downloading the failed files.\" + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download PAM dataset [Optional]\n",
    "The PAM dataset is available at [PAM](https://www.dropbox.com/scl/fo/oxpad3hjijwehrbdp0krf/AHaWA2xhz0lN-x6elq3Sf2I?rlkey=sctuyqczvmqukth4z4lqk2uj7&st=xx2xpwky&dl=0). You can download the dataset using the following command. The dataset is about 14GB. PAM contains the pose aligned 3D models, which is not necessary for the benchmarking if your method does not require the reference 3D models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAM_links = data['PAM']\n",
    "fail_list = []\n",
    "for item, link in PAM_links.items():\n",
    "    print('\\033[93m' + f\"Downloading {item} ...\" + '\\033[0m')\n",
    "    os.makedirs(DATA_PATH + 'PAM', exist_ok=True)\n",
    "    res = download_file(link, f\"{DATA_PATH}PAM/{item}\")\n",
    "    if not res:\n",
    "        fail_list.append(item)\n",
    "if len(fail_list) == 0:\n",
    "    print('\\033[92m' + \"PAM data downloaded successfully!\" + '\\033[0m')\n",
    "else:\n",
    "    print('\\n' + '-'*100)\n",
    "    print('\\033[91m' + \"PAM data download failed for the following files:\" + '\\033[0m')\n",
    "    print(fail_list)\n",
    "    print('\\033[91m' + \"Please rerun the cell to retry downloading the failed files.\" + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Download ROPE dataset\n",
    "The ROPE dataset is available at [ROPE](https://www.dropbox.com/scl/fo/). You can download the dataset using the following command. The dataset contains 363 scenes, with each scene being approximately 500MB. The total size of the dataset is about 180GB. If you want to download a subset of the dataset, you can set the parameter `SCENE_NUM` in the following scripts. If the download is interrupted, you can rerun the script, and it will skip the downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Set the configuration for loading the ROPE data ###################################\n",
    "FILES = ['meta.zip', 'color.zip', 'depth.zip', 'mask.zip', 'mask_sam.zip']   # The files to be downloaded\n",
    "SCENE_NUM = 2                                                                # The number of scenes to be downloaded\n",
    "#######################################################################################################################\n",
    "\n",
    "assert SCENE_NUM > 0 and SCENE_NUM <= 363, 'Invalid scene number'\n",
    "ROPE_links = data['ROPE']\n",
    "scenes = list(ROPE_links.keys())\n",
    "scene_ids = [scene.split('-')[0] for scene in scenes]\n",
    "scene_ids = sorted(list(set(scene_ids)))[:SCENE_NUM]\n",
    "print(f\"The selected scenes are: {scene_ids}\")\n",
    "\n",
    "fail_list = []\n",
    "for index, scene in enumerate(scene_ids):\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    print('\\033[93m' + f\"[{index+1}/{len(scene_ids)}] Downloading scene {scene} ...\" + '\\033[0m')\n",
    "    os.makedirs(DATA_PATH + 'ROPE/' + scene, exist_ok=True)\n",
    "    for file in FILES:\n",
    "        res = download_file(ROPE_links[f\"{scene}-{file}\"], f\"{DATA_PATH}ROPE/{scene}/{file}\")\n",
    "        if not res:\n",
    "            fail_list.append(f\"{scene}-{file}\")\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "if len(fail_list) == 0:\n",
    "    print('\\033[92m' + \"ROPE data downloaded successfully!\" + '\\033[0m')\n",
    "else:\n",
    "    print('\\n' + '-'*100)\n",
    "    print('\\033[91m' + \"ROPE data download failed for the following files:\" + '\\033[0m')\n",
    "    print(fail_list)\n",
    "    print('\\033[91m' + \"Please rerun the cell to retry downloading the failed files.\" + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Download SOPE dataset\n",
    "The SOPE dataset is available at [SOPE](). You can download the dataset using the following command. The dataset comprises 51 patches. If you want to download a subset of the dataset, you can set the `PATCH_NUM` parameter in the scripts below. If the download is interrupted, you can rerun the script, and it will skip the downloaded files.\n",
    "\n",
    "The files `ir.zip`, `depth_1.zip`, and `coord.zip` are not necessary for benchmarking. Specifically, `ir.zip` contains the simulated infrared images, `depth_1.zip` includes the perfect depth images, and `coord.zip` contains the NOCS map proposed in [NOCS](). If you wish to download these files, you can set the `DOWNLOAD_IR`, `DOWNLOAD_GT_DEPTH`, and `DOWNLOAD_NOCS` parameters to `True` in the scripts below. Each batch of the dataset without the optional files is about 14GB. The total size of the dataset is about 720GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Set the configuration for loading the SOPE data ############################################\n",
    "FILES = ['meta.zip', 'color.zip', 'depth.zip', 'depth_1.zip', 'mask.zip', 'coord.zip', 'ir.zip'] # The selected files to be downloaded\n",
    "DOWNLOAD_IR = False                                                                              # Download the IR images\n",
    "DOWNLOAD_GT_DEPTH = False                                                                        # Download the perfect depth images\n",
    "DOWNLOAD_NOCS = False                                                                            # Download the NOCS map\n",
    "PATCH_NUM = 2                                                                                    # The number of patches to be downloaded\n",
    "#########################################################################################################################################\n",
    "\n",
    "FILES.remove('ir.zip') if not DOWNLOAD_IR else None\n",
    "FILES.remove('depth_1.zip') if not DOWNLOAD_GT_DEPTH else None\n",
    "FILES.remove('coord.zip') if not DOWNLOAD_NOCS else None\n",
    "\n",
    "assert PATCH_NUM > 0 and PATCH_NUM <= 50, 'Invalid patch number'\n",
    "SOPE_links = data['SOPE']\n",
    "patches = list(SOPE_links.keys())\n",
    "patch_ids = [patch.split('-')[0] for patch in patches]\n",
    "patch_ids = sorted(list(set(patch_ids)))[:PATCH_NUM]\n",
    "print('\\033[93m' + f\"The total number of patches is: {len(patch_ids)}\" + '\\033[0m')\n",
    "print('\\033[93m' +  f'The selected patches are: {patch_ids}' + '\\033[0m')\n",
    "\n",
    "fail_list = []\n",
    "for index, patch in enumerate(patch_ids):\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    print('\\033[93m' + f\"[{index+1}/{len(patch_ids)}] Downloading patch {patch} ...\" + '\\033[0m')\n",
    "    os.makedirs(DATA_PATH + 'SOPE/' + patch + '/train', exist_ok=True)\n",
    "    os.makedirs(DATA_PATH + 'SOPE/' + patch + '/test', exist_ok=True)\n",
    "    for file in FILES:\n",
    "        res_train = download_file(SOPE_links[f\"{patch}-{'train'}-{file}\"], f\"{DATA_PATH}SOPE/{patch}/train/{file}\")\n",
    "        res_test = download_file(SOPE_links[f\"{patch}-{'test'}-{file}\"], f\"{DATA_PATH}SOPE/{patch}/test/{file}\")\n",
    "        if not res_train:\n",
    "            fail_list.append(f\"{patch}-{'train'}-{file}\")\n",
    "        if not res_test:\n",
    "            fail_list.append(f\"{patch}-{'test'}-{file}\")\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "if len(fail_list) == 0:\n",
    "    print('\\033[92m' + \"SOPE data downloaded successfully!\" + '\\033[0m')\n",
    "else:\n",
    "    print('\\n' + '-'*100)\n",
    "    print('\\033[91m' + \"SOPE data download failed for the following files:\" + '\\033[0m')\n",
    "    print(fail_list)\n",
    "    print('\\033[91m' + \"Please rerun the cell to retry downloading the failed files.\" + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Unzip the files\n",
    "After downloading the files, you can unzip the files using the following command. If you want to remove the zip files after unzipping, you can set the `REMOVE_ZIP` parameter to `True` in the scripts below. The files will be unzipped to the `./data` directory, and the directory structure will be as follows:\n",
    "```\n",
    "data\n",
    "└── Omni6DPose\n",
    "    ├── ROPE\n",
    "    │   ├── SCENE_ID\n",
    "    │   │   ├── FRAME_ID_meta.json\n",
    "    │   │   ├── FRAME_ID_color.png\n",
    "    │   │   ├── FRAME_ID_mask.exr\n",
    "    │   │   ├── FRAME_ID_depth.exr\n",
    "    │   │   ├── FRAME_ID_mask_sam.npz [Optional]\n",
    "    │   │   └── ...\n",
    "    │   └── ...\n",
    "    ├── SOPE\n",
    "    │   ├── PATCH_ID\n",
    "    │   │   ├── train\n",
    "    │   │   │   ├── SCENE_NAME\n",
    "    │   │   │   |  ├── SCENE_ID\n",
    "    │   |   |   |  |   ├── FRAME_ID_meta.json\n",
    "    │   │   │   |  |   ├── FRAME_ID_color.png\n",
    "    │   │   │   |  |   ├── FRAME_ID_mask.exr\n",
    "    │   |   |   |  |   ├── FRAME_ID_depth.exr\n",
    "    │   │   │   |  |   ├── FRAME_ID_depth_1.exr [Optional]\n",
    "    │   │   │   |  |   ├── FRAME_ID_coord.png   [Optional]\n",
    "    │   │   │   |  |   ├── FRAME_ID_ir_l.png    [Optional]\n",
    "    │   │   │   |  |   ├── FRAME_ID_ir_r.png    [Optional]\n",
    "    │   │   │   |  |   └── ...\n",
    "    │   │   │   |  └── ...\n",
    "    │   │   │   └── ...\n",
    "    │   │   └── test\n",
    "    │   └── ...\n",
    "    ├── PAM\n",
    "    │   └── obj_meshes\n",
    "    │       ├── DATASET-CLASS_ID\n",
    "    │       └── ...\n",
    "    └── Meta    \n",
    "        ├── obj_meta.json \n",
    "        └── real_obj_meta.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Set the configuration for unzipping the downloaded files ##################################\n",
    "REMOVE_ZIPS = True   # Remove the zip files after unzipping\n",
    "UNZIP_PAM = True     # Unzip the PAM files\n",
    "UNZIP_ROPE = True    # Unzip the ROPE files\n",
    "UNZIP_SOPE = True    # Unzip the SOPE files\n",
    "###############################################################################################################################\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def unzip_files(directory, remove_zips=False):\n",
    "    files = os.listdir(directory)\n",
    "    for file in files:\n",
    "        if file.endswith('.zip'):\n",
    "            with zipfile.ZipFile(directory + file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(directory)\n",
    "            if remove_zips:\n",
    "                os.remove(directory + file)\n",
    "\n",
    "# Unzip the PAM files\n",
    "if UNZIP_PAM:\n",
    "    print('\\033[93m' + f\"Unzipping files in {DATA_PATH + 'PAM/'} ...\" + '\\033[0m')\n",
    "    unzip_files(DATA_PATH + 'PAM/', remove_zips=REMOVE_ZIPS)\n",
    "\n",
    "# Unzip the ROPE files\n",
    "if UNZIP_ROPE:\n",
    "    scenes = os.listdir(DATA_PATH + 'ROPE/')\n",
    "    print('\\033[93m' + f\"Unzipping files in {DATA_PATH + 'ROPE/'} ...\" + '\\033[0m')\n",
    "    for scene in tqdm(scenes):\n",
    "        unzip_files(DATA_PATH + 'ROPE/' + scene + '/', remove_zips=REMOVE_ZIPS)\n",
    "    \n",
    "# Unzip the SOPE files\n",
    "if UNZIP_SOPE:\n",
    "    patches = os.listdir(DATA_PATH + 'SOPE/')\n",
    "    print('\\033[93m' + f\"Unzipping files in {DATA_PATH + 'SOPE/'} ...\" + '\\033[0m')\n",
    "    for patch in tqdm(patches):\n",
    "        print('\\033[93m' + f\"Unzipping files in {DATA_PATH + 'SOPE/' + patch} ...\" + '\\033[0m')\n",
    "        unzip_files(DATA_PATH + 'SOPE/' + patch + '/train/', remove_zips=REMOVE_ZIPS)\n",
    "        unzip_files(DATA_PATH + 'SOPE/' + patch + '/test/', remove_zips=REMOVE_ZIPS)\n",
    "    print('\\033[92m' + \"All files unzipped successfully!\" + '\\033[0m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omni6dpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
